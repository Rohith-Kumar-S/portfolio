<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Portfolio - Rohith Kumar.S</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="masthead">
        <div class="masthead-inner">
            <div class="site-title">Rohith Kumar.S</div>
            <nav class="nav-links">
                <a href="#about">About Me</a>
                <a href="#projects">Projects</a>
                <a href="#timeline">Timeline</a>
                <a href="#skills">Skills</a>
            </nav>
        </div>
    </div>

    <div class="page-wrapper">
        <aside class="sidebar">
            <div class="profile-card">
                <div class="avatar">
                        <img 
                        src="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/img.png"
                    >
                    </div>
                
                <h3 class="author-name">Rohith Kumar.S</h3>
                <div class="author-bio">M.S. Data Science</div>
                
                <ul class="author-links">
                    <li><a href="#"><i class="fas fa-map-marker-alt" style="width:24px"></i> Northeastern University</a></li>
                    <li><a href="https://drive.google.com/file/d/1tEBC1JHsjufy1zQolUcAacmIWSyboiCF/view?usp=sharing" target="_blank"><i class="fas fa-file" style="width:24px"></i> Resume</a></li>
                    <li><a href="mailto:us.rohithkumar@gmail.com" target="_blank"><i class="fas fa-envelope" style="width:24px"></i> Email</a></li>
                    <li><a href="https://www.linkedin.com/in/rohith-kumar-senthil-kumar-b41207180/" target="_blank"><i class="fab fa-linkedin" style="width:24px"></i> LinkedIn</a></li>
                    <li><a href="https://github.com/Rohith-Kumar-S" target="_blank"><i class="fab fa-github" style="width:24px"></i> Github</a></li>
                </ul>
            </div>
        </aside>

        <main class="main-content">
            
            <section id="about">
                <h1>About Me</h1>
                <p>I am a Master's in Data Science student at Northeastern University, specializing in Deep Learning and Computer Vision. With 3 years of experience in building full-stack and AI solutions, I am well-versed in developing product-specific tools using Core Java and Spring Boot, designing web pages with ReactJS and Angular, and deploying containerized applications using AWS EC2 and Docker.</p>
                <p>Now transitioning into Data Science and Computer Vision, I focus on building end-to-end AI solutions, from computer vision systems for object detection, tracking, and 3D reconstruction to scalable ML applications including recommender systems and forecasting models, supported by robust data pipelines.</p>
            </section>

            <section id="projects">
                <h2>Projects</h2>
                <div class="flex-grid">
                    <article class="project-card" 
                        onclick="openModal(this)"
                        data-title="Multi-domain Adaptive Recommendation System (MARS)"
                        data-start="Aug 2023"
                        data-end="Dec 2023"
                        data-github="https://github.com/Rohith-Kumar-S/MARS"
                        data-tools="Python, PyTorch, pandas, ONNX, K-Means, Pinecone"
                        data-desc="Hybrid recommendation system combining collaborative and content-based filtering, trained on Movie Lens dataset."
                        data-overview="<p>
                       Hybrid recommendation system combining collaborative and content-based filtering, trained on Movie Lens dataset.
                    </p>
                    <p>
                        This project was my first implementation of the collaborative filtering concept, it includes a robust hybrid model that effectively handles the cold start problem.
                    </p>
                    <p>
                        The hybrid approach uses embeddings from the MovieLens dataset, generated with Ollama2 and stored in Pinecone, for content-based filtering, and ranks movies using collaborative filtering results from the ONNX model. The neural collaborative filtering model was trained on the ratings data using PyTorch with GPU acceleration. With an RMSE of over 0.97, it was later converted to ONNX format for faster inference and storage optimization. The original embeddings, which were over 1 GB in size, were reduced to under 100 MB using PCA (from 4096 to 512 dimensions), while still maintaining relevance for similar movie searches. Cosine similarity was applied for similarity search in Pinecone, which stored ~87,000 vectors corresponding to movie ratings in the dataset. The IMDb API was integrated to retrieve additional movie data, which was then used in the user interface built with Streamlit.
                    </p>

                    <p>
                       Disclaimer: The movie images and data used in this project were obtained from publicly available sources, such as MovieLens and the IMDb API, for educational purposes only.
                    </p>
                    <h3>MovieLens Dataset</h3>
                    <p></p>
                    <p>
                        The MovieLens dataset, provided by GroupLens Research at the University of Minnesota, is a widely used benchmark in the field of recommender systems. The dataset is available in various sizes, and the one I picked had 32 Million user-movie ratings across 87K movies.
                    </p>

                    <h3>Movie Genre Co-occurrence</h3>
                    <p></p>
                    <p>Movies in genereal tend to be categorized into multiple genres, and understanding the co-occurrence of these genres can provide insights into user preferences and movie characteristics. Below is a dendrogram visualizing the co-occurrence of different movie genres in the MovieLens dataset.</p>
                    <img src='https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/mars/genres_dendrogram.png' alt='Genre Dendrogram' style='width:50%; height:auto; margin-bottom:1rem; border:1px solid #eee; border-radius:8px;'>
                    <p> The above can be intepreted as like Horror, Crime, Mystery and Thriller genres often co-occur together in the movies with multiple genres. These co-occuring genres were later generalized into broader categories for better analysis and recommendation accuracy.</p>
                    <h3>Architecture for Data and Model Preparation</h3>
                    <p></p>
                    <img src='https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/mars/training_pipeline.png' alt='Architecture Diagram' style='width:100%; height:auto; margin-bottom:1rem; border:1px solid #eee; border-radius:8px;'>
                    <p>Here we store the movie embeddings from Llama2 locally and in Pinecone vector store. We also train the NCF PyTorch model and convert it into ONNX format for faster inference and reduced storage size. </p>
                    
                    <h3>Inference Pipeline</h3>
                    <p></p>
                    <img src='https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/mars/Arch.png' alt='Inference Pipeline' style='width:100%; height:auto; margin-bottom:1rem; border:1px solid #eee; border-radius:8px;'>
                    <p>The core idea is that once a user has rated enough movies, we trigger the inference pipeline. First, we fetch movies similar to those rated by the user using the Pinecone vector store. Next, we infer the user's preferences from their ratings and identify a MovieLens user with the closest taste profile. Finally, using this matched MovieLens user and the similar movies, we rank the movie candidates with an ONNX model to generate the final recommendations.</p>                    
                    <p>Pincone vector store uses the movie embedding from local storage for similarity search. The similar movies fetched from similarity search are then used for content-based filtering. Especially when the user selects a movie to rate, these results populate the 'More like this' section.</p>
                    
                    <h3>Challenges Faced</h3>
                    <p>Designing the user interface and integrating multiple technologies posed significant challenges during the development of the MARS Recommendation System. The interface was built using Streamlit, which limits deployed applications to 1 GB of RAM, while GitHub imposes a 100 MB repository size limit. The dataset alone exceeded 3 GB, and the PyTorch model was too large to store directly. As a result, several model and architectural optimizations were required to make this proof of concept functional on Streamlit Cloud.</p>                    
                    "
                        data-video="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/mars/Mars.mp4"
                    >
                        <div class="card-media"><span><img src="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/mars/Thumbnail.png" alt="Image 1"></span></div>
                        <div class="card-body">
                            <div class="card-title"><a href="javascript:void(0)">MARS Recommendation System</a></div>
                            <div class="card-badges"><span class="badge">PyTorch</span><span class="badge">ONNX</span><span class="badge">K-Means</span><span class="badge">Pinecone</span></div>
                            <div class="card-excerpt">Hybrid movie recommender using collaborative and content-based filtering on the MovieLens dataset.</div>
                            <div class="card-meta"><strong>Topics:</strong> Recommendation Systems, Optimization</div>
                        </div>
                    </article>

                    <article class="project-card" 
                        onclick="openModal(this)"
                        data-title="Bowel Segmenation with SegFormer"
                        data-start="Jan 2023"
                        data-end="May 2023"
                        data-github="https://github.com/Rohith-Kumar-S/BowelSegmentation-MRI"
                        data-tools=" SegFormer, Transformer, Pytorch, OpenCV, NumPy"
                        data-desc="A Vision Pipeline for Bowel Segmentation from MRI Scans using SegFormer Model."
                        data-overview="
                        <p>
                            The goal of this project is to segment different regions of the abdomen using 3D MRI scans of patients undergoing radiotherapy to identify signs of inflammation and bowel damage. </p><p>

                        This was my first segmentation project and introduced me to transfer learning. I experimented with multiple architectures, including VGG16, ResNet50, and attention mechanisms for feature extraction. Initially, a Residual U-Net was trained from scratch, however, due to computational and time constraints, a transfer learning approach was adopted using the SegFormer model. This approach achieved a mean IoU of 75%.
                        </p>
                        <h3>
                            UW-Madison GI Tract Image Segmentation Dataset</h3>
                        <p></p>
                        <p>The dataset contains 467 de-identified MRI scans from 107 patients with each undergone
                        serial (1-5) MRI scans of the abdomen. Every MRI scan consists of approximately 144 axial slices with an average pixel resolution of 1.5 x 1.5 x 3 mm. </p>
                        <p> The dataset contains labeled annotations for five bowel regions: the ampulla of Vater, large bowel, pyloric sphincter, small bowel, and stomach. Of these, only three regions including small bowel, large bowel, and stomach, were used for training, as the remaining classes were underrepresented.</p>
                                                
                        <h3> Training Pipeline</h3>
                        <p></p>
                        <img src='https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/bowel-seg/training_pipeline.png' alt='Training Pipeline' style='width:100%; height:auto; margin-bottom:1rem; border:1px solid #eee; border-radius:8px;'>

                        <p>==============================<br>
                        Per-class IoU Results:<br>
                        ==============================<br>
                        Background: IoU = 0.9876<br>
                        Large Bowel: IoU = 0.6893<br>
                        Small Bowel: IoU = 0.5508<br>
                        Stomach: IoU = 0.7679<br>
                        Mean IoU: 0.7489<br>
                        ==============================</p>
                        
                        <h3> Bowel Obstruction Detection</h3>
                        <p>A bowel obstruction is simply a blockage that prevents contents from moving through. When something blocks the bowel, matter like food, gas, and fluid has nowhere to go. It just keeps accumulating, and that pressure stretches out the bowel wall. So when radiologists spot those swollen, dilated loops on a scan, it's indirect evidence something is blocked.</p>
                        <h3>Obstruction Detection Pipeline</h3> 
                        <p> To detect obstructions, we first segment the bowel regions using the trained SegFormer model. Next, we calculate bowel diameter per slice using distance transform and skeletonization to measure width along the centerline. We then classify risk into 4 tiers (normal, low risk, elevated, high risk) based on clinical thresholds, and visualize results with diameter profile plots and severity heatmaps.<\p>
                        <img src='https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/bowel-seg/detection_pipeline.png' alt='Diameter Profile Plot' style='width:100%; height:auto; margin-bottom:1rem; border:1px solid #eee; border-radius:8px;'>
                        <h3>Diameter Profile Plot</h3> 
                        <p>Following plot is from the results for a test patient from the dataset. Bowel diameter across slices with clinical threshold lines, highlighting regions of concern.</p>
                        <img src='https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/bowel-seg/diameter_plot.png' alt='Diameter Profile Plot' style='width:100%; height:auto; margin-bottom:1rem; border:1px solid #eee; border-radius:8px;'>
                        <h3>Severity Heatmap</h3> 
                        <p>Risk levels across all slices for both small and large bowel.</p>
                        <img src='https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/bowel-seg/severity_plot.png' alt='Severity Heatmap' style='width:100%; height:auto; margin-bottom:1rem; border:1px solid #eee; border-radius:8px;'>
                        "
                        
                        data-image="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/bowel-seg/segmentation_animation.gif"
                        
                     >
                        <div class="card-media"><span><img src="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/bowel-seg/segmentation_animation.gif" alt="Image 1"></span></div>
                        <div class="card-body">
                            <div class="card-title"><a href="javascript:void(0)">Bowel Segmenation with SegFormer</a></div>
                            <div class="card-badges"><span class="badge">Transformer</span><span class="badge">Pytorch</span><span class="badge">OpenCV</span></div>
                            <div class="card-excerpt">Vision Pipeline for Bowel Segmentation from MRI Scans using SegFormer Model.</div>
                            <div class="card-meta"><strong>Topics:</strong> Image Segmentation, Medical Imaging</div>
                        </div>
                    </article>



                    
                    <article class="project-card" 
                        onclick="openModal(this)"
                        data-title="Path Matching"
                        data-start="Jan 2024"
                        data-end="Present"
                        data-github="https://github.com/Rohith-Kumar-S/Path-Matching"
                        data-tools="Python, pandas, Streamlit"
                        data-desc="A interactive simulation tool to visualize shortest path finding and bipartite matching algorithms."
                        data-overview="<p>I started this project as a fun activity to mimic my Algorithm's professor visualization of the A* algorithm during one of his classes. Later, I discovered an interative pathfinding tool, which provides a perfect visualization of the shortest path for a given single sourceâ€“target pair and supports many shortest path algorithms. This inspired me to explore more algorithms and implement them in a similar interactive way. The project goes a step further by finding shortest paths for multi-source and multi-target scenarios and matching sources to targets based on the shortest path between each source target pair. The project is still a work in progress, and I plan to add more algorithms and features in the future. </p>
                        <p>
                        The project features several shortest path finding algorithms, namely Dijkstra's, A*, and Breadth-First Search (BFS). When multiple sources or targets are detected, it incorporates bipartite matching into the process. Bipartite matching can currently be done either using the greedy algorithm or linear programming.
                    </p>
                    <p>
                        For the heuristic in the A* algorithm, the project supports Manhattan, Euclidean, and Diagonal distance calculations. It features a grid-based canvas where users can choose different obstacle maps and define the sources and targets.
                    </p>
                    <p>
                        The red grids indicate sources, and the blue grids indicate targets. The black grids represent obstacles. The yellow grids show the path taken by the algorithm to reach the target from the source, while other colored grids represent the search paths from each source to the target.
                    </p>
                    <p>
                        <b>In terms of applications, this forms the basis for customer-to-taxi matching, delivery-driver-to-order matching, and many more use cases</b>.
                    </p>
                    <p>
                        The project was written in Python and uses the Streamlit library for the web app. The path finding algorithms were implemented from scratch, and the linear programming for bipartite matching was done using the SciPy library.
                    </p>
                        "
                        data-video="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/path-matching/path_matching.mp4"
                    >
                        <div class="card-media"><span><img src="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/path-matching/thumbnail.png" alt="Path Matching"></span></div>
                        <div class="card-body">
                            <div class="card-title"><a href="javascript:void(0)">Path Matching</a></div>
                            <div class="card-badges"><span class="badge">Python</span><span class="badge">pandas</span><span class="badge">Streamlit</span></div>
                            <div class="card-excerpt">A interactive simulation tool to visualize shortest path finding and bipartite matching algorithms.</div>
                            <div class="card-meta"><strong>Topics:</strong> Algorithms, Data Visualization</div>
                        </div>
                    </article>

                    <article class="project-card" 
                        onclick="openModal(this)"
                        data-title="Data Fusion Pipeline"
                        data-start="Jan 2023"
                        data-end="May 2023"
                        data-github="https://github.com/Rohith-Kumar-S/datafusion-pipeline"
                        data-tools="Python, Apache Spark, PostgreSQL, Streamlit"
                        data-desc="A comprehensive rule-based data integration platform that enables users to build and execute scalable data processing pipelines through an intuitive user interface."
                        data-overview="<p>
                        A comprehensive rule-based data integration platform that enables users to build and execute scalable data processing pipelines through an intuitive user interface. The system supports both real-time streaming and batch processing with distributed computing capabilities.
                    </p>
                    <p>
                        I built this as a final project for my Intro to Data Management course here at Northeastern. It is the culmination of the majority of tools I learned during my 3rd semester, covering RDBMS fundamentals, Big Data Systems, and Distributed Computing. I have always wanted to explore the full breadth of a data pipeline, and this project became the perfect sandbox for me to experiment in. I set up the final environment in Docker after countless struggles with manual setup, dealing with Hadoop and PySpark version mismatches which led to the failure of the download feature.
                    </p>
                    <p>
                        At first, the data injection layer only allowed data from Kaggle to be imported, enabling the data to be casted, fused with another Kaggle dataset, and downloaded as a CSV file. Now the application supports Kaggle and GitHub sources through links and streamed data through Apache Kafka. It supports most of the main functionalities available in Spark: Cast, Filter, Rename, Drop, Fill, Explode, Flatten, and Save. It allows users to fuse data by rows and columns based on join type, and supports export options including download, logging to Docker console, and sending to JDBC driver.
                    </p>
                    <p>
                        All the rules the user configures are stored in a MongoDB database, so they can be reused later. The pipelines built can be run in parallel and can be restarted once execution is completed.
                    </p>"
                        data-image="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/data-fusion/1.jpg"
                     >
                        <div class="card-media"><span><img src="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/data-fusion/thumbnail.png" alt="Image 1"></span></div>
                        <div class="card-body">
                            <div class="card-title"><a href="javascript:void(0)">Data Fusion Pipeline</a></div>
                            <div class="card-badges"><span class="badge">Python</span><span class="badge">Apache Spark</span><span class="badge">PostgreSQL</span><span class="badge">Streamlit</span></div>
                            <div class="card-excerpt">Rule-based data integration platform for building scalable pipelines via an intuitive UI.</div>
                            <div class="card-meta"><strong>Topics:</strong> Data Engineering, Streaming, Automation</div>
                        </div>
                    </article>
                    
                     <article class="project-card" 
                        onclick="openModal(this)"
                        data-title="CNN Visualizer"
                        data-start="Jan 2023"
                        data-end="May 2023"
                        data-tools="Python, PyTorch, OpenCV"
                        data-github="https://github.com/Rohith-Kumar-S/cnn-layer-visualizer"
                        data-desc="A comprehensive tool to visualize how CNNs process images."
                        data-overview=" <p>
                        A comprehensive tool to visualize how CNNs process images.
                    </p>
                    <p>
                        I started building this tool as a result of the challenges I faced in understanding how the filters in Convolutional Neural Networks affect input images, specifically while exploring attention U-Nets during a self-supervised approach to segment MRI images of the bowel region.
                    </p>
                    <p>
                        This tool is scalable, with the addition of predefined model weights and image processing capabilities, and can be transformed into a full image processing pipeline with further developments.
                    </p>"
                        data-video="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/cnn-visualizer/cnn_visualizer.mp4"
                     >
                        <div class="card-media"><span><img src="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/cnn-visualizer/thumbnail.png" alt="Image 1"></span></div>
                        <div class="card-body">
                            <div class="card-title"><a href="javascript:void(0)">CNN Visualizer</a></div>
                            <div class="card-badges"><span class="badge">Python</span><span class="badge">PyTorch</span><span class="badge">OpenCV</span></div>
                            <div class="card-excerpt"> A comprehensive tool to visualize how CNNs process images.</div>
                            <div class="card-meta"><strong>Topics:</strong> Neural Networks, Visualization</div>
                        </div>
                    </article>


                    
                    <article class="project-card" 
                        onclick="openModal(this)"
                        data-title="Prime Predict"
                        data-start="Jan 2023"
                        data-end="May 2023"
                        data-tools="Python, XGBoost, CatBoost, K-Means"
                        data-github="https://github.com/Rohith-Kumar-S/PrimePredict"
                        data-desc="Time series forecasting model using Boosting methods for Amazon sales prediction. "
                        data-overview="<p>
                        Amazon's marketplace is a dynamic retail ecosystem generating millions of daily transactions across countless product categories, offering a rich dataset for uncovering real-world patterns in purchase frequency, seasonality, and product performance. These insights are critical to optimizing inventory, pricing, and marketing strategies. To help businesses set revenue goals, allocate budgets, and predict growth across the U.S., this package is organized into four modules that go beyond traditional time series tools like Prophet or ARIMA. By combining regression and tree-ensemble models (XGBoost and CatBoost) with K-Means clustering, it produces accurate point forecasts alongside actionable customer-segment insights.
                    </p>
                    <p>
                        This app was built with the idea that it could also be used by individual manufacturing companies whose products are sold on Amazon and who are trying to add a new product to an existing category. For manufacturers, it provides a quick overview of the performance of a product category. Since time to market is key to sales, they can decide when to manufacture and manage inventory based on demand. Given the forecast date range, the application enables users to forecast overall total sales, total sales for any specified U.S. state, and total sales for any specified product category. When the user clicks the Forecast button, the app first trains the model based on the user's selection if it hasn't already been trained on that specific selection. The trained model is then saved in the 'models/saves' directory. Therefore, if the model has not yet been trained on that specific selection, the user must click Forecast once to train the model and then click Forecast again to retrieve the forecast results. The Streamlit app, written in 'app.py', calls the functions available in 'primepredict.py' to populate the results in the application. If the user performs a forecast based on a state or category, the application first loads the data using DataLoader and generates a processed dataset and stores it in 'data/processed_datasets', which is then used during forecasting. To start the application, install the necessary packages listed in the requirements.txt file and activate the environment. Then, in that environment, navigate to the src folder and run 'streamlit run app.py'. The application will boot up and be ready to use.</p>"
                        data-image="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/prime-predict/1.jpg"
                     >
                        <div class="card-media"><span><img src="https://raw.githubusercontent.com/Rohith-Kumar-S/portfolio/main/images/prime-predict/Thumbnail.png" alt="Image 1"></span></div>
                        <div class="card-body">
                            <div class="card-title"><a href="javascript:void(0)">Prime Predict</a></div>
                            <div class="card-badges"><span class="badge">Python</span><span class="badge">XGBoost</span><span class="badge">CatBoost</span><span class="badge">K-Means</span></div>
                            <div class="card-excerpt">Time series forecasting model using Boosting methods for Amazon sales prediction. </div>
                            <div class="card-meta"><strong>Topics:</strong>Time Series Forecasting, Boosting Methods</div>
                        </div>
                    </article>

                </div>
            </section>

            <section id="timeline">
                <h2>Timeline</h2>
                <ul class="content-list">
                    <li><strong>2021:</strong> Joined Capgemini as a Full-Stack Intern and later transitioned to a full-time role as a Senior Software Engineer.</li>
                    <li><strong>Fall 2022:</strong> Promoted to Associate Consultant, continued working in the Banking and Telecommunications domains.</li>
                    <li><strong>Summer 2023:</strong> Developed a facial recognition system and transitioned toward machine learning and computer vision.</li>
                    <li><strong>Fall 2024:</strong> Began a Master's program in Data Science at Northeastern University.</li>
                    <li><strong>Spring 2026:</strong> Teaching Assistant for the Algorithms course.</li>
                </ul>
            </section>

            <section id="skills">
                <h2>Tools/Skills</h2>
                <div class="skills-grid">
                    <span class="skill-tag">Python</span>
                    <span class="skill-tag">Java</span>
                    <span class="skill-tag">C++</span>
                    <span class="skill-tag">scikit-learn</span>
                    <span class="skill-tag">numpy</span>
                    <span class="skill-tag">pandas</span>
                    <span class="skill-tag">Apache Spark</span>
                    <span class="skill-tag">PyTorch</span>
                    <span class="skill-tag">OpenCV</span>
                    <span class="skill-tag">Git/GitHub</span>
                    <span class="skill-tag">Docker</span>
                    <span class="skill-tag">CI/CD</span>
                    <span class="skill-tag">AWS EC2</span>
                    <span class="skill-tag">AWS RDS</span>
                    <span class="skill-tag">PostgreSQL</span>
                    <span class="skill-tag">MongoDB</span>
                    <span class="skill-tag">Pinecone</span>
                </div>
            </section>

        </main>
    </div>

    <footer class="page-footer">
        <!-- <p>&copy; 2025 Rohith Kumar.S. Powered by Jekyll & AcademicPages.</p> -->
    </footer>
    <div id="projectModal" class="modal">
        <span class="close-btn" onclick="closeModal()">&times;</span>
        
        <div class="modal-content-wrapper">
            
            <div class="modal-header">
                <div class="modal-title" id="m-title">Project Title</div>
                <div class="modal-meta-row">
                    <a id="m-github-link" href="#" target="_blank" class="github-modal-btn">
                        <i class="fab fa-github"></i> View Code
                    </a>
                    <div><i class="far fa-calendar-alt"></i> <span id="m-dates">Jan 2023 - Present</span></div>
                    <div><i class="fas fa-tools"></i> <span id="m-tools-header">Python, React</span></div>

                    
                </div>
            </div>

            <p id="m-desc" style="font-size: 1.3rem; font-weight: 500; margin-bottom: 2rem; color: #555;">
                Brief description goes here...
            </p>

            <div class="modal-main-image" id="m-image">
                [Image/GIF Placeholder]
            </div>

            <h3 style="font-size: 1.8rem; margin-bottom: 1rem;">Overview</h3>
            <div id="m-overview" class="modal-body-text">
                Detailed overview content...
            </div>

            <div class="modal-skills-section">
                <div class="modal-skills-title">Skills & Tools Used</div>
                <div class="skills-grid" id="m-skills-list">
                    </div>
            </div>

        </div>
    </div>
    <script>
        // Helper: Generate a URL-friendly "slug" from a title
        // e.g., "Computer Vision Pipeline" -> "computer-vision-pipeline"
        function createSlug(title) {
            return title.toLowerCase().trim().replace(/ /g, '-').replace(/[^\w-]+/g, '');
        }

        // 1. Check URL on page load to see if we need to open a modal
        window.addEventListener('DOMContentLoaded', () => {
            const params = new URLSearchParams(window.location.search);
            const projectSlug = params.get('project');

            if (projectSlug) {
                const cards = document.querySelectorAll('.project-card');
                // Find the card that matches the slug
                for (let card of cards) {
                    const title = card.getAttribute('data-title');
                    if (createSlug(title) === projectSlug) {
                        openModal(card, false); // false = don't push state (it's already there)
                        break;
                    }
                }
            }
        });

        // 2. Open Modal Function
        function openModal(element, pushToHistory = true) {
            var modal = document.getElementById("projectModal");
            
            // Retrieve data attributes
            var title = element.getAttribute("data-title");
            var start = element.getAttribute("data-start");
            var end = element.getAttribute("data-end");
            var tools = element.getAttribute("data-tools");
            var desc = element.getAttribute("data-desc");
            var overview = element.getAttribute("data-overview");
            var image = element.getAttribute("data-image");
            var video = element.getAttribute("data-video");
            var github = element.getAttribute("data-github");
            // Populate Modal DOM
            document.getElementById("m-title").innerText = title;
            document.getElementById("m-dates").innerText = start + " - " + end;
            document.getElementById("m-tools-header").innerText = tools;
            document.getElementById("m-desc").innerText = desc;
            document.getElementById("m-overview").innerHTML = overview;
            
            var imgContainer = document.getElementById("m-image");
            if(image!=null) {
                if (image.includes("Placeholder")) {
                    imgContainer.innerText = image;
                    imgContainer.style.backgroundImage = "none";
                } else {
                    imgContainer.innerHTML = `<img src="${image}" style="width:100%; height:100%; object-fit:cover; border-radius:8px;">`;
                }
            }

            if (video!=null) {
                imgContainer.innerHTML = `<video controls style="width:100%; height:100%; border-radius:8px;"><source src="${video}" type="video/mp4">Your browser does not support the video tag.</video>`;
            }

            var githubBtn = document.getElementById("m-github-link");
            if (github) {
                githubBtn.href = github;
                githubBtn.style.display = "inline-flex";
            } else {
                githubBtn.style.display = "none"; // Hide if no link provided
            }

            // Populate Skills at Bottom
            var skillsContainer = document.getElementById("m-skills-list");
            skillsContainer.innerHTML = ""; 
            var toolsArray = tools.split(",");
            toolsArray.forEach(function(tool) {
                var span = document.createElement("span");
                span.className = "skill-tag";
                span.innerText = tool.trim();
                skillsContainer.appendChild(span);
            });

            // Show Modal
            modal.style.display = "block";
            document.body.style.overflow = "hidden";

            // Update URL if requested
            if (pushToHistory) {
                const slug = createSlug(title);
                const newUrl = window.location.pathname + '?project=' + slug + window.location.hash;
                history.pushState({ modalOpen: true }, '', newUrl);
            }
        }

        // 3. Close Modal Function
        function closeModal() {
            var modal = document.getElementById("projectModal");
            modal.style.display = "none";
            document.body.style.overflow = "auto";
            
            // Clean up URL: Remove the ?project=... query param but keep the hash (#projects)
            const cleanUrl = window.location.pathname + window.location.hash;
            history.pushState(null, '', cleanUrl);
        }

        // 4. Handle Browser Back Button
        window.onpopstate = function(event) {
            // If the user presses back, we just close the modal visually
            // (The URL is already updated by the browser's back action)
            var modal = document.getElementById("projectModal");
            if (modal.style.display === "block") {
                modal.style.display = "none";
                document.body.style.overflow = "auto";
            }
        };

        // Close if clicked outside content
        window.onclick = function(event) {
            var modal = document.getElementById("projectModal");
            if (event.target == modal) {
                closeModal();
            }
        }
    </script>
</body>
</html>